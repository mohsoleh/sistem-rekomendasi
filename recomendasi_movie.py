# -*- coding: utf-8 -*-
"""recomendasi_movie.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MAEr2Rw-3QMCE-dXt5qz4-33yG78l8kh

# Proyek Akhir: Membuat Model Sistem Rekomendasi

* Domain : Movie
* Tujuan : Membangun Model *Machine Learning* untuk Rekomendasi Film
* Dataset yang digunakan : https://www.kaggle.com/datasets/parasharmanas/movie-recommendation-system

## DATA UNDERSTANDING

### Load Dataset

Dataset merupakan data yang diperoleh melalui website GroupLens dengan judul MovieLens 25M Dataset. Langkah awal kita download terlebih dahulu dataset dengan kode berikut.
"""

# loads library
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

import kagglehub
import shutil

# ignore all future warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""Kode ini memuat pustaka pandas, matplotlib, dan NumPy untuk manipulasi data, visualisasi, dan komputasi numerik. Sedangkan Kagglehub digunakan untuk mengakses dataset Kaggle, dan shutil menangani operasi file."""

# Menentukan path di mana dataset akan diunduh
custom_path = "/content"

# Download dataset ke path yang ditentukan
path = kagglehub.dataset_download("parasharmanas/movie-recommendation-system")

# Tujuan path
destination_dir = "/content"

# Menyalin semua isi folder
shutil.copytree(path, destination_dir, dirs_exist_ok=True)

"""Kode ini mengunduh dataset Kaggle ke direktori /path menggunakan kagglehub dan menyalinnya ke direktori /content dengan shutil.copytree. Selanjutnya mari kita lihat jumlah data pada masing-masing dataset, menggunakan kode berikut :"""

movies = pd.read_csv('/content/movies.csv') #data film
ratings = pd.read_csv('/content/ratings.csv') #data ratings film

# check and count unique values in each dataframes
print('Jumlah data film: ', len(movies.movieId.unique()))
print('Jumlah data ratings: ', len(ratings.userId.unique()))

"""Jumlah data dalam dataset film cukup besar yaitu sekitar 62.423 data, sedangkan untuk ratings terdapat 162.541 data. sehingga sangat mendukung pengembangan model rekomendasi. Selanjutnya, mari kita melakukan eksplorasi terhadap data dengan teknik univariate exploratory data analysis.

## Univariate Exploratory Data Analysis

Mari kita lihat informasi dari dataset dengan menggunakan function `info()`
"""

# movies information
movies.info()

"""Dataset ini terdiri dari 62.423 baris dengan 3 kolom utama, yaitu movieId, title, dan genres. Selanjutnya mari kita lihat informasi rating menggunakan perintah berikut :"""

print('Banyak genre: ', len(movies.genres.unique()))
print('Tipe genre: ', movies.genres.unique())

"""Dataset mencakup 1.639 kombinasi genre unik, dengan beberapa tipe genre seperti Adventure|Animation|Children|Comedy|Fantasy, Comedy|Romance, dan kombinasi lainnya yang mencakup horor, misteri, hingga fiksi ilmiah, menunjukkan keragaman genre dalam dataset."""

# ratings information
ratings.info()

"""Dataset ini memiliki ukuran data yang sangat besar yakni 762.9 MB, dan memiliki 4 kolom diantaranya adalah userId, movieId, rating dan timestamp. Selanjutnya mari kita lihat informasi dari rating lebih detail lagi, menggunakan perintah berikut:"""

ratings.head()

ratings.describe()

"""Dari data di ratings memiliki rata-rata rating sebesar 3.53. Sebagian besar rating berkisar antara 3.0 sampai 4.0, yang mencerminkan bahwa mayoritas film mendapatkan penilaian yang cukup baik. Rating terendah adalah 0.5, sementara yang tertinggi mencapai 5.0."""

print('Jumlah userId: ', len(ratings.userId.unique()))
print('Jumlah movieId: ', len(ratings.movieId.unique()))
print('Jumlah data rating: ', len(ratings))

"""Kode ini menghitung jumlah pengguna sebanyak 162.541, film 59.047 data, dan total data rating dalam dataset sebanyak 25.000.095, data ini memberikan gambaran tentang skala interaksi antara pengguna dan film.

# Data Preprocessing
"""

# dataset films
movies

"""Data yang ditampilkan merupakan sebuah dataset yang berisi informasi tentang film, dengan tiga kolom utama: movieId, title, dan genres. Dataset ini memiliki 62.423 baris yang mencakup berbagai film dari berbagai genre. Dari data di atas mari kita pisah tahun rilis dengan title untuk menghindari terjadinya redundansi data, menggunakan kode berikut :"""

#separate tittles by year of release
movies['year_of_release'] = movies.title.str.extract('([0-9]{4})')
movies.head()

"""Ok, berhasil memisah tahun rilis dan title dengan menambahkan satu kolom year_of_release. sekarang kita hapus tahun rilis pada title menggunakan kode berikut :"""

# Convert column to string
movies['title'] = movies['title'].astype(str)

# Remove year from title
movies['title'] = movies['title'].str.split('(', n=1).str[0].str.strip()
movies

"""Sekarang sudah terlihat rapi, dengan memisah tahun rilis dan title dengan menambahkan satu kolom year_of_release. berikutnya mari kita lihat data ratings."""

# dataset user ratings
ratings

"""Dataset di atas memberikan informasi tentang rating yang diberikan oleh pengguna pada berbagai film, dengan empat kolom utama: userId, movieId, rating, dan timestamp. Dataset ini mencakup 25.000.095 baris, yang menunjukkan interaksi antara pengguna dan film dengan berbagai rating yang diberikan. selanjutnya mari kita konversi timestamp menjadi datetime, menggunakan kode berikut:"""

# unit='s' to convert it into epoch time
import datetime

ratings.timestamp = pd.to_datetime(ratings['timestamp'], unit='s')
ratings.head()

"""Selanjutnya kita gabungkan kedua datafreame movies dan ratings menjadi satu dataframe yang utuh."""

# merge dataframe
films = pd.merge(movies, ratings, on='movieId', how='left')
films

"""Penggabungan ini menghasilkan data sebanyak 25.003.471 dari data sebelumnya yang masing-masing data movie hanya 62.423 dan data rating sebanyak 25.000.095

# Data Preparation

Setelah proses penggabungan, mari kita cek lagi datanya apakah ada missing value atau tidak. menggunakan kode berikut :
"""

# check missing values
films.isnull().sum()

"""Terdapat banyak missing value pada sebagian besar fitur. Hanya fitur movieId, title, ganres saja yang memiliki 0 missing value. Sehingga perlu kita drop, menggunakan kode berikut :"""

# drop missing values
films = films.dropna()
films

"""Perhatikanlah, data kita sekarang memiliki 24.988.552 baris dari data sebelumnya yang sekitas 25.003.471. tandanya data yang kosong (missing value) sudah berhasil dihapus. Untuk memastikan tidak ada missing value lagi dalam data, kita cek kode berikut:"""

# recheck missing values
films.isnull().sum()

"""Nah, sudah bersih. Selanjutnya tadi di data movie ada yang tidak memiliki genre, untuk memastikannya mari kita cek data movie yang tidak memiliki gendre dengan menggunakan kode berikut :"""

# show non-genre
films[films['genres']=='(no genres listed)']

"""Terdapat 23.726 data yang tidak memiliki genre. karena datanya tidak terlalu besar mari kita hapus data tersebut menggunakan kode berikut:"""

# clean non-genre
films = films[(films.genres != '(no genres listed)')]
films

"""Dari data 24.988.552 sebelumnya, sekarang data menjadi 24.964.826 yang tandanya semua movie yang tidak memiliki genre sudah berhasil dihapus, mari kita lanjutkan ke tahapan berikutnya, yakni kita kita bulatkan angka pada rating yang sebelumnya berada di range nilai terkecil 0.5 dan nilai terbesarnya 5.0, untuk mempermudah pengkategoriannya. Sehingga rating memiliki nilai 1-5."""

# round values
films = films.copy()
films['rating'] = films['rating'].apply(np.ceil)
films

"""Sepintas data rating sudah dibulatkan, untuk memastikan kembali mari kita cek menggukan kode berikut :"""

# check values in rating columns
films.rating.unique()

films.rating.describe()

"""Dari data di atas dapat kita lihat rating terkecil adalah 1 dan terbesar adalah 5, dengan sebaran data yang bervariasi ditunjukkan dari nilai Standar deviasi sebesar 1.06. Selanjutnya mari kita cek duplikasi datanya."""

# duplicated by movieId
films.duplicated('movieId').sum()

"""Data yang ada berjumlah sekitar 24.964.826, terdapat duplikasi data berdasarkan `movieId` sebanyak 24.910.474."""

# duplicated by title
films.duplicated('title').sum()

"""Sedangkan duplikasi data berdasarkan `title` movie sebanyak 24.914.219. Selanjutnya mari kita hapus semua data duplikat menggunakan kode berikut:"""

# drop duplicated data by movieId & title
films = films.drop_duplicates('movieId')
films = films.drop_duplicates('title')

"""Asik ternyata tidak ada data yang duplikat, selanjutnya kita cek pada data genre, menggunakan kode berikut:"""

# show genre films
import sys

np.set_printoptions(threshold=sys.maxsize)
print('Banyak genre films: ', len(films['genres'].unique()))
print('Genre films: ', films['genres'].unique())
np.set_printoptions(threshold=None)

"""Hasil dari kode di atas, kita melihat genre file bernama 'Sci-Fi', yang merupakan singkatan dari 'Science Fiction' yang berarti film tersebut merupakan film fiksi ilmiah. Karena 'Sci-Fi' menggunakan tanda pisah (dash), hal ini perlu dihapus (tanda pemisahnya - dash). Jika tidak, saat tahap vektorisasi menggunakan TF-IDF, kata 'Sci-Fi' akan dianggap sebagai dua kata terpisah, yaitu 'Sci' dan 'Fi', yang bisa memengaruhi hasil analisis."""

# replace the matching strings for 'sci-fi' using regex
films = films.replace(to_replace ='[nS]ci-Fi', value = 'Scifi', regex = True)
films

# Cari genre 'Sci-Fi'
sci_fi_count = films[films['genres'].str.contains('Sci-Fi', case=False)].shape[0]

print(f"Jumlah genre 'Sci-Fi': {sci_fi_count}")

"""Ok, sudah tidak ada genre Sci-Fi pada data kita, selanjutnya mari kita lihat distribusi pada data Rating"""

# Membuat variabel preparation yang berisi dataframe films kemudian mengurutkan berdasarkan movieId
preparation = films
preparation.sort_values('movieId')

"""Selanjutnya, kita perlu melakukan konversi data series menjadi list. Dalam hal ini, kita menggunakan fungsi tolist() dari library numpy. Implementasikan kode berikut."""

# Convert data series ‘movieId’ to list form
film_id = preparation['movieId'].tolist()

# Convert data series ‘title’ to list form
film_name = preparation['title'].tolist()

# Convert data series ‘genres’ to list form
film_genre = preparation['genres'].tolist()

print(len(film_id))
print(len(film_name))
print(len(film_genre))

"""Kode ini mengonversi kolom movieId, title, dan genres dari dataset preparation ke dalam bentuk daftar Python menggunakan metode .tolist(). Variabel film_id, film_name, dan film_genre masing-masing menyimpan data ID film, judul film, dan genre film dalam format `list`. Ketiga kolom di atas memiliki jumlah data yang sama yakni 50.607. Tahap berikutnya, kita akan membuat dictionary untuk menentukan pasangan key-value pada data film_id, film_name, dan genre yang telah kita siapkan sebelumnya."""

# Create dataframe using dict form of ‘film_id’, ‘film_name’, and ‘film_genre’
df_film = pd.DataFrame({
    'film_id': film_id,
    'film_name': film_name,
    'genre': film_genre
})

df_film

"""Sampai tahap ini, data telah siap untuk dimasukkan ke dalam pemodelan.

# Model Development dengan Content Based Filtering

Saatnya kita mengembangkan sistem rekomendasi dengan pendekatan *content based filtering*. Tapi sebelumnya, mari cek lagi data yang kita miliki dan assign dataframe dari tahap sebelumnya ke dalam variabel data, sebagai berikut:
"""

# data sample
data = df_film
data.sample(5)

"""## TF-IDF Vectorizer

Pada tahap ini, kita akan membangun sistem rekomendasi sederhana berdasarkan genre suatu film
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data cuisine
tf.fit(data['genre'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

"""Selanjutnya, lakukan fit dan transformasi ke dalam bentuk matriks."""

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['genre'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Perhatikanlah, matriks yang kita miliki berukuran (50607, 20). Nilai 50607 merupakan ukuran data dan 20 merupakan matrik genre.

Untuk menghasilkan vektor tf-idf dalam bentuk matriks, kita menggunakan fungsi todense(). Jalankan kode berikut.
"""

# change tf-idf vector to matrix form
tfidf_matrix.todense()

"""Selanjutnya, mari kita lihat matriks tf-idf untuk beberapa film (film_name) dan genre (genre). Terapkan kode berikut."""

# Membuat Sparse DataFrame
sparse_df = pd.DataFrame.sparse.from_spmatrix(
    tfidf_matrix,
    index=data['film_name'],             # Nama film sebagai index
    columns=tf.get_feature_names_out()  # Genre sebagai kolom
)

# Sampling pada baris dan kolom
sampled_df = sparse_df.sample(10, axis=0).sample(20, axis=1)
sampled_df

"""Kode ini membuat Sparse DataFrame dari matriks TF-IDF (tfidf_matrix) untuk menghubungkan film dengan genre secara efisien, menggunakan nama film sebagai indeks dan genre sebagai kolom. Selanjutnya, dilakukan sampling untuk memilih 10 baris (film) dan 20 kolom (genre) secara acak, menghasilkan subset kecil data yang siap untuk dianalisis lebih lanjut.

## Cosine Similarity

Pada tahap sebelumnya, kita telah berhasil mengidentifikasi korelasi antara film dengan gendre. Sekarang, kita akan menghitung derajat kesamaan (similarity degree) antar film dengan teknik `cosine similarity`. Di sini, kita menggunakan fungsi `cosine_similarity` dari library sklearn.
"""

# calculate cosine similarity on matrix
from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix, dense_output=False)

"""Kode ini menghitung cosine similarity dari matriks TF-IDF (tfidf_matrix) menggunakan fungsi cosine_similarity dari pustaka sklearn. Cosine similarity mengukur kesamaan antar film berdasarkan representasi TF-IDF-nya, menghasilkan nilai antara 0 hingga 1, di mana 1 menunjukkan kesamaan maksimum. Parameter dense_output=False memastikan hasilnya tetap dalam format matriks sparse untuk efisiensi memori."""

# Konversi sparse matrix ke dense
cosine_sim_dense = cosine_sim.toarray()

# Membuat DataFrame dari dense matrix
cosine_sim_df = pd.DataFrame(
    cosine_sim_dense,
    index=data['film_name'],
    columns=data['film_name']
)

print('Shape:', cosine_sim_df.shape)

# show similarity matrix
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Kode ini mengonversi matriks cosine similarity dari bentuk sparse ke bentuk dense menggunakan toarray(), sehingga dapat diproses lebih lanjut. Selanjutnya, matriks dense ini digunakan untuk membuat sebuah DataFrame bernama cosine_sim_df, di mana indeks dan kolomnya adalah nama-nama film dari data['film_name'].

Dengan cosine_sim_df.shape, ukuran matriks ditampilkan sebagai (50607, 50607), menunjukkan bahwa ada 50.607 film dengan nilai kesamaan antarfilm. sample(5, axis=1).sample(10, axis=0) menampilkan subset kecil dari matriks kesamaan untuk pengamatan.

## Mendapatkan Rekomendasi

Sebelumnya, kita telah memiliki data similarity (kesamaan) antar film. selanjutnya kita buat rekomendasi film kepada pengguna, menggunakan kode berikut:
"""

# function recommendations
def film_recommendations(film_name, similarity_data=cosine_sim_df, items=data[['film_name', 'genre']], k=5):
    # get data index
    index = similarity_data.loc[:,film_name].to_numpy().argpartition(
        range(-1, -k, -1))

    # retrieve data from an existing index
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # drop film_name you want to search
    closest = closest.drop(film_name, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

# check data
data[data.film_name.eq('Toy Story')]

"""Kode ini memeriksa dataset data untuk mencari baris yang memiliki nama film `Toy Story`"""

# get recommendations
film_recommendations('Toy Story')

"""Berhasil! Sistem kita memberikan rekomendasi 5 film dengan genre Adventure|Animation|Children|Comedy|Fantasy

# Model Development dengan Collaborative Filtering

Sebelumnya kita telah menerapkan teknik content based filtering pada data. Teknik ini merekomendasikan item yang mirip dengan preferensi pengguna di masa lalu. Berikutnya, kita akan menerapkan teknik `collaborative filtering` untuk membuat sistem rekomendasi. Teknik ini membutuhkan data rating dari user. Sebelum itu mari kita melakukan impor library berikut :
"""

# Import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""Sebelumnya, kita telah membuat variabel `preparation` dan menetapkan data pada variabel tersebut. Untuk memudahkan supaya tidak tertukar pada data, kita ubah nama variabel `preparation` menjadi df."""

# read dataset
df = preparation
df

"""Perhatikanlah, data di atas memiliki 50.607 baris dan 7 kolom.

## Data Preparation

Kini Anda memasuki tahap preprocessing. Pada tahap ini, Anda perlu melakukan persiapan data untuk menyandikan (encode) fitur ‘userId’ dan 'movieId' ke dalam indeks integer. Terapkan kode berikut.
"""

# change unique values of 'userId' to list
user_ids = df['userId'].unique().tolist()
print('list userID: ', user_ids)

# encode 'userId'
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# encoding index to 'userId'
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""Kode ini melakukan encoding pada kolom userId dalam dataset. Pertama, nilai unik dari userId diubah menjadi daftar menggunakan .unique().tolist(). Kemudian, setiap userId di-encode menjadi indeks numerik menggunakan dictionary user_to_user_encoded. Sebaliknya, dictionary user_encoded_to_user memetakan indeks numerik kembali ke userId asli. Ini memungkinkan representasi userId dalam bentuk numerik yang lebih mudah digunakan dalam model dan analisis."""

# change unique values of 'movieId' to list
films_ids = df['movieId'].unique().tolist()

# encode 'movieId'
films_to_films_encoded = {x: i for i, x in enumerate(films_ids)}

# encoding index to 'movieId'
films_encoded_to_films = {i: x for i, x in enumerate(films_ids)}

"""Kode di atas melakukan encoding pada kolom movieId dalam dataset. Pertama, nilai unik dari movieId diubah menjadi daftar menggunakan .unique().tolist(). Kemudian, setiap movieId dipetakan ke indeks numerik melalui dictionary films_to_films_encoded. Sebaliknya, dictionary films_encoded_to_films memungkinkan pemetaan indeks numerik kembali ke movieId asli. Proses ini memungkinkan representasi movieId dalam bentuk numerik yang lebih mudah untuk digunakan dalam analisis atau model pembelajaran mesin. Berikutnya, petakan userID dan movieID ke dataframe yang berkaitan."""

# Mapping 'userId' to dataframe
df['user'] = df['userId'].map(user_to_user_encoded)

# Mapping 'movieId' ke dataframe
df['films'] = df['movieId'].map(films_to_films_encoded)

"""Terakhir, cek beberapa hal dalam data seperti jumlah user, jumlah film, dan mengubah nilai rating menjadi float."""

# get number of users
num_users = len(user_to_user_encoded)
print(num_users)

# get number of films
num_films = len(films_encoded_to_films)
print(num_films)

# change dtype
df['rating'] = df['rating'].values.astype(np.float32)

# get min values of rating
min_rating = min(df['rating'])

# get max values of rating
max_rating = max(df['rating'])

print('Number of User: {}, Number of Films: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_films, min_rating, max_rating
))

"""## Membagi Data untuk Training dan Validasi

Pada tahap ini kita akan melakukan pembagian data menjadi data training dan validasi. Namun sebelumnya, acak datanya terlebih dahulu agar distribusinya menjadi random.
"""

# sampling
df = df.sample(frac=1, random_state=42)
df

"""Selanjutnya, kita bagi data train dan validasi dengan komposisi 80:20. Namun sebelumnya, kita perlu memetakan (mapping) data user dan film menjadi satu value terlebih dahulu. Lalu, buatlah rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training."""

# mapping users and films data into one value
x = df[['user', 'films']].values

# ratings
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# split data train and validation with 80/20 composition
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Proses Training

Selanjutnya kita membuat class RecommenderNet dengan keras Model class. Kode class RecommenderNet ini di ambil dari situs Dicoding.
"""

# class recommendations
class RecommenderNet(tf.keras.Model):

  # __init__
  def __init__(self, num_users, num_films, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_films = num_films
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.films_embedding = layers.Embedding( # layer embeddings films
        num_films,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.films_bias = layers.Embedding(num_films, 1) # layer embedding films bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # layer embedding 2
    films_vector = self.films_embedding(inputs[:, 1]) # layer embedding 3
    films_bias = self.films_bias(inputs[:, 1]) # layer embedding 4

    dot_user_films = tf.tensordot(user_vector, films_vector, 2)

    x = dot_user_films + user_bias + films_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""Selanjutnya, lakukan proses compile terhadap model."""

model = RecommenderNet(num_users, num_films, 50) # model initialization

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Model ini menggunakan Binary Crossentropy untuk menghitung loss function, Adam (Adaptive Moment Estimation) sebagai optimizer, dan root mean squared error (RMSE) sebagai metrics evaluation. Selanjutnya, mari kita mulai proses training."""

# training
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 32,
    epochs = 25,
    validation_data = (x_val, y_val)
)

"""## Visualisasi Metrik

Untuk melihat visualisasi proses training, mari kita plot metrik evaluasi dengan matplotlib. Menggunakan kode berikut.
"""

# plot metrics evaluations
plt.plot(history.history['root_mean_squared_error'], color='blue')
plt.plot(history.history['val_root_mean_squared_error'], color='red')
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""Berdasarkan hasil log pelatihan, terlihat bahwa nilai root_mean_squared_error (RMSE) untuk data pelatihan terus menurun secara konsisten dari 0.2790 (epoch 1) menjadi 0.1353 (epoch 25). Hal ini menunjukkan bahwa model berhasil mempelajari pola pada data pelatihan dengan baik. Untuk data validasi, nilai RMSE juga mengalami penurunan, meskipun cenderung melambat setelah beberapa epoch awal dan stabil di sekitar nilai 0.2371 pada epoch terakhir.

## Mendapatkan Rekomendasi

Untuk mendapatkan rekomendasi film, pertama kita ambil sampel user secara acak dan definisikan variabel film_not_visited yang merupakan daftar film yang belum pernah dikunjungi oleh pengguna.
"""

# films
films_df = df_film
films_df.head()

# data ratings
df = films
df.head()

# taking user samples
user_id = df.userId.sample(1).iloc[0]
films_visited_by_user = df[df.userId == user_id]

# bitwise operators (~), can be found here https://docs.python.org/3/reference/expressions.html
films_not_visited = films_df[~films_df['film_id'].isin(films_visited_by_user.movieId.values)]['film_id']
films_not_visited = list(
    set(films_not_visited)
    .intersection(set(films_to_films_encoded.keys()))
)

films_not_visited = [[films_to_films_encoded.get(x)] for x in films_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_films_array = np.hstack(
    ([[user_encoder]] * len(films_not_visited), films_not_visited)
)

"""Selanjutnya, untuk memperoleh rekomendasi film, gunakan fungsi model.predict() dari library Keras dengan menerapkan kode berikut."""

# get recommendations
ratings = model.predict(user_films_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_films_ids = [
    films_encoded_to_films.get(films_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('films with high ratings from user')
print('----' * 8)

top_films_user = (
    films_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

films_df_rows = films_df[films_df['film_id'].isin(top_films_user)]
for row in films_df_rows.itertuples():
    print(row.film_name, ':', row.genre)

print('----' * 8)
print('Top 10 films recommendation')
print('----' * 8)

recommended_films = films_df[films_df['film_id'].isin(recommended_films_ids)]
for row in recommended_films.itertuples():
    print(row.film_name, ':', row.genre)

"""Horee, sudah berhasil memberikan rekomendasi kepada user. Hasil di atas adalah rekomendasi untuk user dengan id 3. Dari output tersebut, kita dapat membandingkan antara `films with high ratings from user` dan `Top 10 films recommendation untuk user.`"""